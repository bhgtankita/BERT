{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Accessing Llama 13b chat model using hosted API from Replicate\n\n- Llama 7B, 13B, 70B parameters models\n- Pretrained + chat","metadata":{}},{"cell_type":"markdown","source":"# Accessing Llama2\n\n1. Download + self host (on-premise)\n2. Hosted API platform (e.g. Replicate)\n3. Hosted Container Platform (e.g. Azure, AWS, GCP)","metadata":{}},{"cell_type":"code","source":"# presentation layer code\n\nimport base64\nfrom IPython.display import Image, display\nimport matplotlib.pyplot as plt\n\ndef mm(graph):\n    graphbytes = graph.encode(\"ascii\")\n    base64_bytes = base64.b64encode(graphbytes)\n    base64_string = base64_bytes.decode(\"ascii\")\n    display(Image(url=\"https://mermaid.ink/img/\" + base64_string))\n\ndef genai_app_arch():\n    mm(\"\"\"\n  flowchart TD\n    A[Users] --> B(Applications e.g. mobile, web)\n    B --> |Hosted API|C(Platforms e.g. Custom, HuggingFace, Replicate)\n    B -- optional --> E(Frameworks e.g. LangChain)\n    C-->|User Input|D[Llama 2]\n    D-->|Model Output|C\n    E --> C\n    classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n  \"\"\")\n\ndef rag_arch():\n    mm(\"\"\"\n  flowchart TD\n    A[User Prompts] --> B(Frameworks e.g. LangChain)\n    B <--> |Database, Docs, XLS|C[fa:fa-database External Data]\n    B -->|API|D[Llama 2]\n    classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n  \"\"\")\n\ndef llama2_family():\n    mm(\"\"\"\n  graph LR;\n      llama-2 --> llama-2-7b\n      llama-2 --> llama-2-13b\n      llama-2 --> llama-2-70b\n      llama-2-7b --> llama-2-7b-chat\n      llama-2-13b --> llama-2-13b-chat\n      llama-2-70b --> llama-2-70b-chat\n      classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n  \"\"\")\n\ndef apps_and_llms():\n    mm(\"\"\"\n  graph LR;\n    users --> apps\n    apps --> frameworks\n    frameworks --> platforms\n    platforms --> Llama 2\n    classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n  \"\"\")\n\nimport ipywidgets as widgets\nfrom IPython.display import display, Markdown\n\n# Create a text widget\nAPI_KEY = widgets.Password(\n    value='',\n    placeholder='',\n    description='API_KEY:',\n    disabled=False\n)\n\ndef md(t):\n    display(Markdown(t))\n\ndef bot_arch():\n    mm(\"\"\"\n  graph LR;\n  user --> prompt\n  prompt --> i_safety\n  i_safety --> context\n  context --> Llama_2\n  Llama_2 --> output\n  output --> o_safety\n  i_safety --> memory\n  o_safety --> memory\n  memory --> context\n  o_safety --> user\n  classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n  \"\"\")\n\ndef fine_tuned_arch():\n    mm(\"\"\"\n  graph LR;\n      Custom_Dataset --> Pre-trained_Llama\n      Pre-trained_Llama --> Fine-tuned_Llama\n      Fine-tuned_Llama --> RLHF\n      RLHF --> |Loss:Cross-Entropy|Fine-tuned_Llama\n      classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n  \"\"\")\n\ndef load_data_faiss_arch():\n    mm(\"\"\"\n  graph LR;\n      documents --> textsplitter\n      textsplitter --> embeddings\n      embeddings --> vectorstore\n      classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n  \"\"\")\n\ndef mem_context():\n    mm(\"\"\"\n      graph LR\n      context(text)\n      user_prompt --> context\n      instruction --> context\n      examples --> context\n      memory --> context\n      context --> tokenizer\n      tokenizer --> embeddings\n      embeddings --> LLM\n      classDef default fill:#CCE6FF,stroke:#84BCF5,textColor:#1C2B33,fontFamily:trebuchet ms;\n  \"\"\")","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:19:54.788949Z","iopub.execute_input":"2023-10-04T04:19:54.789503Z","iopub.status.idle":"2023-10-04T04:19:54.930365Z","shell.execute_reply.started":"2023-10-04T04:19:54.789436Z","shell.execute_reply":"2023-10-04T04:19:54.929160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%pip install -qU \\\n    replicate \\\n    langchain \\\n    sentence_transformers \\\n    pdf2image \\\n    pdfminer \\\n    pdfminer.six \\\n    unstructured \\\n    faiss-gpu","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:19:54.933048Z","iopub.execute_input":"2023-10-04T04:19:54.933763Z","iopub.status.idle":"2023-10-04T04:21:01.359976Z","shell.execute_reply.started":"2023-10-04T04:19:54.933721Z","shell.execute_reply":"2023-10-04T04:21:01.358617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llama2_family()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:21:01.362189Z","iopub.execute_input":"2023-10-04T04:21:01.362592Z","iopub.status.idle":"2023-10-04T04:21:01.376191Z","shell.execute_reply.started":"2023-10-04T04:21:01.362555Z","shell.execute_reply":"2023-10-04T04:21:01.372719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# llama 23b chat model hosted on replicate server\nllama2_13b = \"meta/llama-2-13b-chat:f4e2de70d66816a838a89eeeb621910adffb0dd0baba3976c96980970978018d\"","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:21:01.379984Z","iopub.execute_input":"2023-10-04T04:21:01.380311Z","iopub.status.idle":"2023-10-04T04:21:01.392354Z","shell.execute_reply.started":"2023-10-04T04:21:01.380284Z","shell.execute_reply":"2023-10-04T04:21:01.391055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from getpass import getpass\nimport os\n\nREPLICATE_API_TOKEN = getpass()\nos.environ[\"REPLICATE_API_TOKEN\"] = REPLICATE_API_TOKEN # r8_CiCtDLiET5uO0YdjqLo4mahUNz7BomS0QliwQ","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:21:01.393244Z","iopub.execute_input":"2023-10-04T04:21:01.393526Z","iopub.status.idle":"2023-10-04T04:21:20.592514Z","shell.execute_reply.started":"2023-10-04T04:21:01.393502Z","shell.execute_reply":"2023-10-04T04:21:20.591358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import replicate\n\n# text completion with input prompt\ndef Completion(prompt):\n    output = replicate.run(\n        llama2_13b,\n        input={\"prompt\": prompt, \"max_new_tokens\":1000}\n    )\n    return \"\".join(output)\n\n# chat completion with input prompt and system prompt\ndef ChatCompletion(prompt, system_prompt=None):\n    output = replicate.run(\n        llama2_13b,\n        input={\n            \"system_prompt\": system_prompt,\n            \"prompt\": prompt,\n            \"max_new_tokens\": 1000\n        }\n    )\n    return \"\".join(output)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:21:20.594692Z","iopub.execute_input":"2023-10-04T04:21:20.595187Z","iopub.status.idle":"2023-10-04T04:21:21.100817Z","shell.execute_reply.started":"2023-10-04T04:21:20.595147Z","shell.execute_reply":"2023-10-04T04:21:21.099378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Basic Completion","metadata":{}},{"cell_type":"code","source":"output = Completion(\n    prompt=\"The typical color of a llama is: \"\n)\nmd(output)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:21:21.102199Z","iopub.execute_input":"2023-10-04T04:21:21.103392Z","iopub.status.idle":"2023-10-04T04:21:27.153557Z","shell.execute_reply.started":"2023-10-04T04:21:21.103358Z","shell.execute_reply":"2023-10-04T04:21:27.152416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# System prompts","metadata":{}},{"cell_type":"code","source":"output = ChatCompletion(\n    prompt = \"The typical color of a llama is: \",\n    system_prompt = \"respond with only one word\"\n)\nmd(output)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:21:27.155029Z","iopub.execute_input":"2023-10-04T04:21:27.155598Z","iopub.status.idle":"2023-10-04T04:21:28.636528Z","shell.execute_reply.started":"2023-10-04T04:21:27.155563Z","shell.execute_reply":"2023-10-04T04:21:28.634978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output = ChatCompletion(\n    prompt=\"The typical color of a llama is: \",\n    system_prompt=\"response in json format\"\n)\nmd(output)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:21:28.638747Z","iopub.execute_input":"2023-10-04T04:21:28.639682Z","iopub.status.idle":"2023-10-04T04:21:30.746803Z","shell.execute_reply.started":"2023-10-04T04:21:28.639640Z","shell.execute_reply":"2023-10-04T04:21:30.745742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Gen AI Application Architecture","metadata":{}},{"cell_type":"code","source":"genai_app_arch()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:21:30.750335Z","iopub.execute_input":"2023-10-04T04:21:30.750669Z","iopub.status.idle":"2023-10-04T04:21:30.758842Z","shell.execute_reply.started":"2023-10-04T04:21:30.750642Z","shell.execute_reply":"2023-10-04T04:21:30.758148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Chatbot Architecture","metadata":{}},{"cell_type":"code","source":"bot_arch()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:21:30.760095Z","iopub.execute_input":"2023-10-04T04:21:30.760519Z","iopub.status.idle":"2023-10-04T04:21:30.846324Z","shell.execute_reply.started":"2023-10-04T04:21:30.760450Z","shell.execute_reply":"2023-10-04T04:21:30.845295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Chat conversation","metadata":{}},{"cell_type":"markdown","source":"- LLMs are stateless\n- Single turn\n- Multi turn (Memory)","metadata":{}},{"cell_type":"code","source":"# Single turn chat\nprompt_chat = \"What is the average lifespan of a Llama?\"\noutput = ChatCompletion(prompt=prompt_chat, system_prompt=\"answer the last question in few words\")\nmd(output)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:21:30.847480Z","iopub.execute_input":"2023-10-04T04:21:30.848478Z","iopub.status.idle":"2023-10-04T04:21:32.330744Z","shell.execute_reply.started":"2023-10-04T04:21:30.848446Z","shell.execute_reply":"2023-10-04T04:21:32.329643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# without previous context\nprompt_chat = \"What animal family are they?\"\noutput = ChatCompletion(prompt=prompt_chat, system_prompt=\"answer the last question in few words\")\nmd(output)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:21:32.331816Z","iopub.execute_input":"2023-10-04T04:21:32.333071Z","iopub.status.idle":"2023-10-04T04:21:33.807255Z","shell.execute_reply.started":"2023-10-04T04:21:32.333005Z","shell.execute_reply":"2023-10-04T04:21:33.806200Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Multi-turn chat\nprompt_chat = \"\"\"\nUser: What is the average lifespan of a Llama?\nAssistant: Sure! The average lifespan of a llama is around 20-30 years.\nUser: What animal family are they?\n\"\"\"\noutput = ChatCompletion(prompt=prompt_chat, system_prompt=\"answer the last question\")\nmd(output)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:21:33.808852Z","iopub.execute_input":"2023-10-04T04:21:33.809262Z","iopub.status.idle":"2023-10-04T04:21:35.291187Z","shell.execute_reply.started":"2023-10-04T04:21:33.809224Z","shell.execute_reply":"2023-10-04T04:21:35.290042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prompt Engineering\n\n- Prompt Emgineering refers to the science of designing effective prompts to get desired responses\n- Help reduce hallucination","metadata":{}},{"cell_type":"markdown","source":"# In-context Learning (e.g. zero-shot, few-shot)\n- In-context learning - specific method of prompt engineering where demonstration of task are provided as part of prompt\n1. zero-shot learning - model is performing tasks without any input examples\n2. few or N-shot learning - model is performing and behaving based on input examples in user's prompt","metadata":{}},{"cell_type":"code","source":"# zero-shot example\nprompt = \"\"\"\nClassify: I saw a Gecko.\nSntiment: ?\n\"\"\"\noutput = ChatCompletion(prompt, system_prompt=\"one word response\")\nmd(output)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:21:35.292717Z","iopub.execute_input":"2023-10-04T04:21:35.293019Z","iopub.status.idle":"2023-10-04T04:21:36.784551Z","shell.execute_reply.started":"2023-10-04T04:21:35.292993Z","shell.execute_reply":"2023-10-04T04:21:36.783198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# by giving examples to Llama, it \nprompt = \"\"\"\nClassify: I love Llamas!\nSentiment: Positive\nClassify: I dont like Snakes.\nSentiment: Negative\nClassify: I saw a Gecko.\nSentiment:\n\"\"\"\noutput = ChatCompletion(prompt, system_prompt=\"one word response\")\nmd(output)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:21:36.786262Z","iopub.execute_input":"2023-10-04T04:21:36.786720Z","iopub.status.idle":"2023-10-04T04:21:38.279350Z","shell.execute_reply.started":"2023-10-04T04:21:36.786677Z","shell.execute_reply":"2023-10-04T04:21:38.278144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# another zero-shot learning\nprompt = \"\"\"\nQuestion: Vicuna?\nAnswer:\n\"\"\"\noutput = ChatCompletion(prompt, system_prompt=\"one word response\")\nmd(output)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:21:38.280683Z","iopub.execute_input":"2023-10-04T04:21:38.280975Z","iopub.status.idle":"2023-10-04T04:21:39.768785Z","shell.execute_reply.started":"2023-10-04T04:21:38.280949Z","shell.execute_reply":"2023-10-04T04:21:39.767479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Another few-shot learning example with formatted prompt.\n\nprompt = '''\nQUESTION: Llama?\nANSWER: Yes\nQUESTION: Alpaca?\nANSWER: Yes\nQUESTION: Rabbit?\nANSWER: No\nQUESTION: Vicuna?\nANSWER:'''\n\noutput = ChatCompletion(prompt, system_prompt=\"one word response\")\nmd(output)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:21:39.770391Z","iopub.execute_input":"2023-10-04T04:21:39.770684Z","iopub.status.idle":"2023-10-04T04:21:41.233090Z","shell.execute_reply.started":"2023-10-04T04:21:39.770659Z","shell.execute_reply":"2023-10-04T04:21:41.231991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Chain of thoughts\n- enables complex reasoning through logical step by step thinking and generates meaningful and contextually relevant responses","metadata":{}},{"cell_type":"code","source":"# Standard prompting\nprompt = '''\nLlama started with 5 tennis balls. It buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does Llama have now?\n'''\n\noutput = ChatCompletion(prompt, system_prompt=\"provide short answer\")\nmd(output)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:21:41.234506Z","iopub.execute_input":"2023-10-04T04:21:41.235046Z","iopub.status.idle":"2023-10-04T04:21:43.315548Z","shell.execute_reply.started":"2023-10-04T04:21:41.235017Z","shell.execute_reply":"2023-10-04T04:21:43.314781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Chain-Of-Thought prompting\nprompt = '''\nLlama started with 5 tennis balls. It buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does Llama have now?\nLet's think step by step.\n'''\n\noutput = ChatCompletion(prompt, system_prompt=\"provide short answer\")\nmd(output)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:21:43.316584Z","iopub.execute_input":"2023-10-04T04:21:43.317592Z","iopub.status.idle":"2023-10-04T04:21:47.885515Z","shell.execute_reply.started":"2023-10-04T04:21:43.317526Z","shell.execute_reply":"2023-10-04T04:21:47.884211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Retrieval Augmented Generation (RAG)\n- RAG allows to retrieve snippets of information from extrenal data sources and augment it to the user's prompt to get tailored responses from Llama2\n- For our demo, we are going to download an external PDF file from a URL and query against the content in the pdf file to get contextually relevant information back with the help of Llama!","metadata":{}},{"cell_type":"code","source":"rag_arch()","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:21:47.887387Z","iopub.execute_input":"2023-10-04T04:21:47.887760Z","iopub.status.idle":"2023-10-04T04:21:47.894825Z","shell.execute_reply.started":"2023-10-04T04:21:47.887732Z","shell.execute_reply":"2023-10-04T04:21:47.893665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LangChain\n\n- is a framework that helps make it easier to implement RAG","metadata":{}},{"cell_type":"code","source":"# langchain setup\nfrom langchain.llms import Replicate\n\n# Use the Llama 2 model hosted on Replicate\n# Temperature: Adjusts randomness of outputs, greater than 1 is random and 0 is deterministic, 0.75 is a good starting value\n# top_p: When decoding text, samples from the top p percentage of most likely tokens; lower to ignore less likely tokens\n# max_new_tokens: Maximum number of tokens to generate. A word is generally 2-3 tokens\nllama_model = Replicate(\n    model=llama2_13b,\n    model_kwargs={\"temperature\": 0.75, \"top_p\": 1, \"max_new_tokens\":1000}\n)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:21:47.896166Z","iopub.execute_input":"2023-10-04T04:21:47.896949Z","iopub.status.idle":"2023-10-04T04:21:49.413321Z","shell.execute_reply.started":"2023-10-04T04:21:47.896917Z","shell.execute_reply":"2023-10-04T04:21:49.411968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Step 1: load the external data source. In our case, we will load Meta’s “Responsible Use Guide” pdf document.\nfrom langchain.document_loaders.pdf import OnlinePDFLoader\nloader = OnlinePDFLoader(\"https://ai.meta.com/static-resource/responsible-use-guide/\")\ndocuments = loader.load()\n\n# Step 2: Get text splits from document\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\ntext_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\nall_splitter = text_splitter.split_documents(documents)\n\n# Step 3: Use the embedding model\nfrom langchain.vectorstores import FAISS\nfrom langchain.embeddings import HuggingFaceEmbeddings\nmodel_name = \"sentence-transformers/all-mpnet-base-v2\" # embedding model\nmodel_kwargs = {\"device\": \"cpu\"}\nembeddings = HuggingFaceEmbeddings(model=model_name, model_kwargs=model_kwargs)\n\n# Step 4: Use vector store to store embeddings\nvectorstore = FAISS.from_documents(all_splits, embeddings)","metadata":{"execution":{"iopub.status.busy":"2023-10-04T04:31:11.324121Z","iopub.execute_input":"2023-10-04T04:31:11.324580Z","iopub.status.idle":"2023-10-04T04:31:12.504244Z","shell.execute_reply.started":"2023-10-04T04:31:11.324548Z","shell.execute_reply":"2023-10-04T04:31:12.501974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# LangChain Q&A Retriever\n- ConversationalRetrievalChain\n- Query the Source documents","metadata":{}},{"cell_type":"code","source":"# Query against your own data\nfrom langchain.chains import ConversationalRetrievalChain\nchain = ConversationalRetrievalChain.from_llm(llama_model, vectorstore.as_retriever(), return_source_documents=True)\n\nchat_history = []\nquery = \"How is Meta approaching open science in two short sentences?\"\nresult = chain({\"question\": query, \"chat_history\": chat_history})\nmd(result['answer'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This time your previous question and answer will be included as a chat history which will enable the ability\n# to ask follow up questions.\nchat_history = [(query, result[\"answer\"])]\nquery = \"How is it benefiting the world?\"\nresult = chain({\"question\": query, \"chat_history\": chat_history})\nmd(result['answer'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Fine-Tuning Models\n- Limitatons of Prompt Eng and RAG\n- Fine-Tuning Arch\n- Types (PEFT, LoRA, QLoRA)\n- Using PyTorch for Pre-Training & Fine-Tuning\n- Evals + Quality","metadata":{}},{"cell_type":"code","source":"fine_tuned_arch()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}